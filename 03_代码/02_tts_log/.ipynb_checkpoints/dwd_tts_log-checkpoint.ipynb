{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c481597c-25ce-4a4b-8be1-986349a486c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import get_json_object, regexp_extract, col, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae6ccb5-0821-4b5b-be52-475c0f4e13db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/20 13:48:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"spark://hdp01:7077\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hdp01:9083\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"hdfs://htwcluster/warehouse\") \\\n",
    "        .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "        .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n",
    "        .appName(\"dwd_tts_log\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "161f68eb-da0d-49a0-99a6-a2d3e89096e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载log数据\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "# log_df = spark.read.text(\"hdfs://htwcluster/warehouse/ods/flume/robot-tts/2024-06-18\")\n",
    "log_df = spark.read.text(\"hdfs://htwcluster/warehouse/ods/flume/robot-tts/{}\".format(today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1c333d1-2144-482f-8f33-bdb4888d8fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(value='03:09:56.689 INFO  c.h.t.a.RobotUploadDataController - [getVoiceData,46] - {\"vin\":\"HTWW232204A000069\",\"content\":\"系统连接成功\",\"voiceCompany\":\"IFLYTEK\",\"scene\":\"app2-1_box\",\"voiceType\":1,\"voiceName\":\"x2_xiaojuan\",\"engineType\":1,\"time\":\"2024-01-10 03:09:58\"}')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "265faec1-3c77-439f-8f8b-7c6ceb87d420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- log_value: string (nullable = true)\n",
      " |-- opera_time: string (nullable = true)\n",
      " |-- json_str: string (nullable = true)\n",
      " |-- vin: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- voice_company: string (nullable = true)\n",
      " |-- scene: string (nullable = true)\n",
      " |-- voice_type: string (nullable = true)\n",
      " |-- voice_name: string (nullable = true)\n",
      " |-- engine_type: string (nullable = true)\n",
      " |-- create_time: string (nullable = true)\n",
      " |-- create_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from dwd.dwd_platformlogs_externaltts_forever_inc\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "785699ee-cc8f-48c0-817f-7b84a796560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据清洗-非结构化处理\n",
    "log_df.withColumns({\n",
    "    \"opera_time\":col('value').substr(0,12),\\\n",
    "    \"json_str\":regexp_extract(col('value'),'\\{.*\\}',0),\\\n",
    "    \"vin\":get_json_object(col('json_str'), '$.vin'),\\\n",
    "    \"content\":get_json_object(col('json_str'), '$.content'),\\\n",
    "    \"voice_company\":get_json_object(col('json_str'), '$.voiceCompany'),\\\n",
    "    \"scene\":get_json_object(col('json_str'), '$.scene'),\\\n",
    "    \"voice_type\":get_json_object(col('json_str'), '$.voiceType'),\\\n",
    "    \"voice_name\":get_json_object(col('json_str'), '$.voiceName'),\\\n",
    "    \"engine_type\":get_json_object(col('json_str'), '$.engineType'),\\\n",
    "    \"create_time\":get_json_object(col('json_str'), '$.time'),\\\n",
    "    \"create_date\":date_format(col('create_time'),'yyyy-MM-dd'),\\\n",
    "}).dropDuplicates().createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "288ce80a-5b3b-427c-b826-e63dc42fb5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据清洗-null值处理\n",
    "# spark.sql(\"select * from df\").describe().toPandas().T # 查看数据情况。 数据情况良好，没有空值\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5123b856-2d6b-4530-8fc2-4c4198442cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据比较去重写入\n",
    "spark.sql(\"select value from dwd.dwd_platformlogs_externaltts_forever_inc\").createOrReplaceTempView(\"df_hive\")\n",
    "sql = \"\"\"\n",
    "select \n",
    "    df.*\n",
    "from df\n",
    "left join df_hive\n",
    "    on df.value = df_hive.value\n",
    "where df_hive.value is null\n",
    "\"\"\"\n",
    "df = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7b4f90e-db2f-464c-b3bd-6a0cced3f37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 数据写入\n",
    "df.write.format(\"hive\").partitionBy('create_date').mode(\"append\").saveAsTable(\"dwd.dwd_platformlogs_externaltts_forever_inc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94c3d8d1-73b6-41e7-ad62-6bc2dee1c245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建表结构\n",
    "sql = \"\"\"\n",
    "create table if not exists dwd.dwd_platformlogs_externaltts_forever_inc(\n",
    "    value string comment '日志原始值' ,\n",
    "    opera_time string comment '操作时间' ,\n",
    "    json_str string comment 'json文本' ,\n",
    "    vin string comment '设备码' ,\n",
    "    content string comment '语音内容' ,\n",
    "    voice_company string comment '语音服务方' ,\n",
    "    scene string comment '场景' ,\n",
    "    voice_type string comment '语音类型' ,\n",
    "    voice_name string comment '语音包名' ,\n",
    "    engine_type string comment '引擎类型' ,\n",
    "    create_time string comment '创建时间' \n",
    ")\n",
    "comment 'dwd_语音内容'\n",
    "partitioned by (create_date string)\n",
    "stored as orc\n",
    "\"\"\"\n",
    "# spark.sql(\"drop table if exists dwd.dwd_platformlogs_externaltts_forever_inc\")\n",
    "# spark.sql(sql)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
