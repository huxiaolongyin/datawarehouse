{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c58d434-db6f-46c6-992d-a500506859d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import get_json_object, regexp_extract, col, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c8f97a4-fe2c-43ed-b25a-e93742251467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/19 14:06:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/19 14:06:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"spark://hdp01:7077\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hdp01:9083\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"hdfs://htwcluster/warehouse\") \\\n",
    "        .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "        .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n",
    "        .appName(\"dwd_aiui_log\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9f83d6a-c519-4d03-b0fd-6d5511a58578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载log数据\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "# log_df = spark.read.text(\"hdfs://htwcluster/warehouse/ods/flume/aiui-reprocessing/2024-06-18\")\n",
    "log_df = spark.read.text(\"hdfs://htwcluster/warehouse/ods/flume/aiui-reprocessing/{}\".format(today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9ec36a8-5b13-417a-b847-75d03c7fe52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据清洗-非结构化处理\n",
    "log_df.withColumns({\"opera_time\":col('value').substr(0,12),\\\n",
    "                    \"json_str\":regexp_extract(col('value'),'\\{.*\\}',0),\\\n",
    "                    \"vin\":get_json_object(col('json_str'), '$.vin'),\\\n",
    "                    \"create_time\":get_json_object(col('json_str'), '$.createTime'),\\\n",
    "                    \"msg_type\":get_json_object(col('json_str'), '$.Msg.Type'),\\\n",
    "                    \"service_type\":get_json_object(col('json_str'), '$.Msg.ContentJson.intent.serviceType'),\\\n",
    "                    \"msg_text\":get_json_object(col('json_str'), '$.Msg.ContentJson.intent.text'),\\\n",
    "                    \"rc\":get_json_object(col('json_str'), '$.Msg.ContentJson.intent.rc'),\\\n",
    "                    \"topic_id\":get_json_object(col('json_str'), '$.Msg.ContentJson.intent.answer.topicID'),\\\n",
    "                    \"emotion\":get_json_object(col('json_str'), '$.Msg.ContentJson.intent.answer.emotion'),\\\n",
    "                    \"question_text\":get_json_object(col('json_str'), '$.Msg.ContentJson.intent.answer.question.question'),\\\n",
    "                    \"answer_type\":get_json_object(col('json_str'), '$.Msg.ContentJson.intent.answer.answerType'),\\\n",
    "                    \"answer_text\":get_json_object(col('json_str'), '$.Msg.ContentJson.intent.answer.text'),\\\n",
    "                    \"msg_uuid\":get_json_object(col('json_str'), '$.Msg.ContentJson.intent.uuid'),\\\n",
    "                    \"msg_sid\":get_json_object(col('json_str'), '$.Msg.ContentJson.intent.sid'),\\\n",
    "                    \"service_name\":get_json_object(col('json_str'), '$.Msg.ContentJson.intent.serviceName'),\\\n",
    "                    \"operation\":get_json_object(col('json_str'), '$.Msg.ContentJson.intent.operation'),\\\n",
    "                    \"mark\":get_json_object(col('json_str'), '$.mark'),\\\n",
    "                    \"create_date\":date_format(col('create_time'),'yyyy-MM-dd'),\\\n",
    "                   }).dropDuplicates().createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "191bae2c-f0d1-4c89-93ae-70ff4afedef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "107715"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据清洗-null值处理\n",
    "# spark.sql(\"select * from df\").describe().toPandas().T # 查看数据情况\n",
    "sql = \"\"\"\n",
    "select\n",
    "    value,\n",
    "    opera_time,\n",
    "    json_str,\n",
    "    coalesce(vin,'-99') as vin,\n",
    "    create_time,\n",
    "    msg_type,\n",
    "    service_type,\n",
    "    msg_text, \n",
    "    coalesce(rc,'-99') as rc,\n",
    "    if(topic_id=='null',null,topic_id) as topic_id,\n",
    "    emotion,\n",
    "    question_text,\n",
    "    answer_type,\n",
    "    answer_text,\n",
    "    msg_uuid,\t\n",
    "    msg_sid,\n",
    "    service_name,\n",
    "    operation,\n",
    "    coalesce(mark,'1') as mark,\n",
    "    create_date\n",
    "from df\n",
    "\"\"\"\n",
    "spark.sql(sql).createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92e9a06f-fbfb-42fa-91ee-277335198f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据比较去重写入\n",
    "spark.sql(\"select value from dwd.dwd_platformlogs_aiui_forever_inc\").createOrReplaceTempView('df_hive')\n",
    "sql = \"\"\"\n",
    "select \n",
    "    df.*\n",
    "from df\n",
    "left join df_hive\n",
    "    on df.value = df_hive.value\n",
    "where df_hive.value is null\n",
    "\"\"\"\n",
    "df = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ffe1674-fd4e-4564-9dec-d0482a3b0c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 数据写入\n",
    "df.write.format(\"hive\").partitionBy('create_date').mode(\"append\").saveAsTable(\"dwd.dwd_platformlogs_aiui_forever_inc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "658ffaa6-b1ef-4653-bcd0-8aa2020f1f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/19 14:50:08 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建表结构\n",
    "sql = \"\"\"\n",
    "create table if not exists dwd.dwd_platformlogs_aiui_forever_inc(\n",
    "    value string comment '日志原始值' ,\n",
    "    opera_time string comment '操作时间' ,\n",
    "    json_str string comment 'json文本' ,\n",
    "    vin string comment '设备码' ,\n",
    "    create_time string comment '创建时间' ,\n",
    "    msg_type string comment '消息类型' ,\n",
    "    service_type string comment '服务类型' ,\n",
    "    msg_text string comment '消息内容' ,\n",
    "    rc string comment '' ,\n",
    "    topic_id string comment '话题id' ,\n",
    "    emotion string comment '表情' ,\n",
    "    question_text string comment '问题内容' ,\n",
    "    answer_type string comment '回答类型' ,\n",
    "    answer_text string comment '问题内容' ,\n",
    "    msg_uuid string comment '消息uuid' ,\n",
    "    msg_sid string comment '消息sid' ,\n",
    "    service_name string comment '服务商名称' ,\n",
    "    operation string comment '操作类型' ,\n",
    "    mark string comment '是否汉特云命中' \n",
    ")\n",
    "comment 'dwd_ai服务接口调用日志明细表'\n",
    "partitioned by (create_date string)\n",
    "stored as orc\n",
    "\"\"\"\n",
    "# spark.sql(\"drop table if exists dwd.dwd_platformlogs_aiui_forever_inc\")\n",
    "# spark.sql(sql)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
