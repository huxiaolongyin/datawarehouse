{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "542ee525-af2a-43df-8afd-0e6df0a028f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a7b73f-980a-4557-b475-73262278d48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/19 16:30:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"spark://hdp01:7077\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hdp01:9083\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"hdfs://htwcluster/warehouse\") \\\n",
    "        .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "        .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n",
    "        .appName(\"ads_aiui_log\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a635f4ae-0ac3-4d1d-9e63-380e70e9bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = '192.168.30.101'\n",
    "port = 3306\n",
    "user = 'root'\n",
    "password = 'Han2Te0Win-19'\n",
    "database = 'base'\n",
    "table='aiui_platform_statistical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c45d380-bbb5-48e1-9bcb-5cadc1cd8908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/19 16:30:52 WARN HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does not exist\n"
     ]
    }
   ],
   "source": [
    "# 统计指标\n",
    "sql=\"\"\"\n",
    "select \n",
    "    vin,\n",
    "    create_date as statistical_date,\n",
    "    count(*) as statistical_num,\n",
    "    '' as remark,\n",
    "    mark,\n",
    "    cast(current_timestamp() as string) as correct_time\n",
    "from dwd.dwd_platformlogs_aiui_forever_inc\n",
    "group by vin, create_date, mark\n",
    "\"\"\"\n",
    "spark.sql(sql).createOrReplaceTempView('df_hive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c482825-3798-4f8d-a01d-d5f60f4b7908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去重标记\n",
    "# 读取\n",
    "url = 'jdbc:mysql://192.168.30.101:3306/base?useUnicode=true&characterEncoding=utf8&zeroDateTimeBehavior=convertToNull&useSSL=false&serverTimezone=GMT%2B8'\n",
    "spark.read.jdbc(url, table=table, properties={\"user\":user, \"password\":password}).createOrReplaceTempView('df_mysql')\n",
    "\n",
    "sql = \"\"\"\n",
    "select\n",
    "    t1.*,\n",
    "    case\n",
    "        when t1.statistical_num != t2.statistical_num then 'update'\n",
    "        when t1.statistical_num is not null and t2.statistical_num is null then 'insert'\n",
    "        else 'no_change'\n",
    "    end as operation\n",
    "from df_hive t1\n",
    "left join df_mysql t2\n",
    "on t1.vin=t2.vin and t1.statistical_date=t2.statistical_date and t1.mark=t2.mark\n",
    "\"\"\"\n",
    "spark.sql(sql).createOrReplaceTempView('df_res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a02c65bc-dcfd-449f-8da3-070962413b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 写入数据\n",
    "df_insert = spark.sql(\"select * from df_res where operation='insert'\")\n",
    "df_update = spark.sql(\"select * from df_res where operation='update'\")\n",
    "\n",
    "insert_cols = df_insert.columns[:-1]\n",
    "\n",
    "# 转化为列表\n",
    "insert_data = [(row.vin, row.statistical_date, row.statistical_num, row.remark, row.mark, row.correct_time) for row in  df_insert.select(insert_cols).collect()]\n",
    "update_data = [(row.statistical_num, row.correct_time, row.vin, row.statistical_date, row.mark) for row in df_update.collect()]\n",
    "\n",
    "# 统计值, 用于后续判断写入情况\n",
    "insert_count = df_insert.count()\n",
    "update_count = df_update.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8deee54-b68e-48db-a301-a327cb123893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写入数据\n",
    "connection = pymysql.connect(\n",
    "        host=host,\n",
    "        port=port,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        database=database\n",
    ")\n",
    "    \n",
    "insert_query = \"\"\"\n",
    "        INSERT INTO {} ({})\n",
    "        VALUES ({})\n",
    "        \"\"\".format(table, ', '.join(insert_cols), ('%s, '*len(insert_cols))[:-2])\n",
    "\n",
    "update_query = \"\"\"\n",
    "        UPDATE {}\n",
    "        SET  statistical_num=%s, correct_time=%s\n",
    "        WHERE vin=%s and statistical_date=%s and mark=%s\n",
    "        \"\"\".format(table)\n",
    "\n",
    "with connection.cursor() as cursor:\n",
    "    # 插入操作\n",
    "    if insert_count == 0:\n",
    "        pass\n",
    "    elif insert_count == 1:\n",
    "        cursor.execute(insert_query, insert_data)\n",
    "    else:\n",
    "        cursor.executemany(insert_query, insert_data)\n",
    "\n",
    "    # 更新操作\n",
    "    if update_count == 0:\n",
    "        pass\n",
    "    elif update_count == 1:\n",
    "        cursor.execute(update_query, update_data)\n",
    "    else:\n",
    "        cursor.executemany(update_query, update_data)\n",
    "        \n",
    "# 提交事务\n",
    "connection.commit()\n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
