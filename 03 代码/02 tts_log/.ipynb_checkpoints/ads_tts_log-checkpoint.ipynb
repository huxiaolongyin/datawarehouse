{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d7e66b5-9d53-4993-ae3b-cf8b4d4e3cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d5018a-02c3-4479-8bdd-10590c055b84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition.mode\n",
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "Warning: Ignoring non-Spark config property: hive.exec.dynamic.partition\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/20 14:41:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"spark://hdp01:7077\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hdp01:9083\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"hdfs://htwcluster/warehouse\") \\\n",
    "        .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "        .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\\\n",
    "        .appName(\"ads_tts_log\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfccdedb-734b-4cfb-9fc3-28a750f2cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = '192.168.30.101'\n",
    "port = 3306\n",
    "user = 'root'\n",
    "password = 'Han2Te0Win-19'\n",
    "database = 'base'\n",
    "table='tts_platfrom_statistical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d947ba0c-1009-433f-8bbf-c5fe75614ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 14:41:38 WARN HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does not exist\n"
     ]
    }
   ],
   "source": [
    "# 统计指标\n",
    "sql = \"\"\"\n",
    "select \n",
    "    vin,\n",
    "    create_date as statistical_date,\n",
    "    count(*) as statistical_num,\n",
    "    content,\n",
    "    cast(current_timestamp() as string) as correct_time,\n",
    "    voice_name as voiceName,\n",
    "    voice_type as voiceType,\n",
    "    engine_type as engineType\n",
    "from dwd.dwd_platformlogs_externaltts_forever_inc\n",
    "group by vin,create_date,content,voice_name,voice_type,engine_type\n",
    "\"\"\"\n",
    "spark.sql(sql).createOrReplaceTempView('df_hive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63daecab-7f6b-4257-bcb5-d272507f61d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去重标记\n",
    "# 读取\n",
    "url = 'jdbc:mysql://192.168.30.101:3306/base?useUnicode=true&zeroDateTimeBehavior=convertToNull&useSSL=false&serverTimezone=GMT%2B8'\n",
    "spark.read.jdbc(url, table=table, properties={\"user\":user, \"password\":password}).createOrReplaceTempView('df_mysql')\n",
    "\n",
    "sql = \"\"\"\n",
    "select\n",
    "    t1.*,\n",
    "    case\n",
    "        when t1.statistical_num != t2.statistical_num then 'update'\n",
    "        when t1.statistical_num is not null and t2.statistical_num is null then 'insert'\n",
    "        else 'no_change'\n",
    "    end as operation\n",
    "from df_hive t1\n",
    "left join df_mysql t2 \n",
    "on t1.vin=t2.vin and t1.statistical_date=t2.statistical_date and t1.content=t2.content and t1.voiceName=t2.voiceName and t1.voiceType=t2.voiceType and t1.engineType=t2.engineType\n",
    "\"\"\"\n",
    "spark.sql(sql).createOrReplaceTempView('df_res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf140296-905c-4ac5-8aa7-75ba43fc7b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 写入数据\n",
    "df_insert = spark.sql(\"select * from df_res where operation='insert'\")\n",
    "df_update = spark.sql(\"select * from df_res where operation='update'\")\n",
    "\n",
    "insert_cols = df_insert.columns[:-1]\n",
    "\n",
    "# 转化为列表\n",
    "insert_data = [(row.vin, row.statistical_date, row.statistical_num, row.content, row.correct_time, row.voiceName, row.voiceType, row.engineType) for row in  df_insert.select(insert_cols).collect()]\n",
    "update_data = [(row.statistical_num, row.correct_time, row.vin, row.statistical_date, row.content, row.voiceName, row.voiceType, row.engineType) for row in df_update.collect()]\n",
    "\n",
    "# 统计值, 用于后续判断写入情况\n",
    "insert_count = df_insert.count()\n",
    "update_count = df_update.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24aa218e-4d36-4712-810c-4439da919bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 14:43:06 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-01ada0e4-54ce-4b23-aacb-f4e4fa4ad425. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-01ada0e4-54ce-4b23-aacb-f4e4fa4ad425\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:173)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2120)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2310)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2310)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2216)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:686)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "24/06/20 14:43:06 WARN NioEventLoop: Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@d5b601.\n",
      "24/06/20 14:43:06 WARN NioEventLoop: Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@6ec013fd.\n"
     ]
    }
   ],
   "source": [
    "# 写入数据\n",
    "connection = pymysql.connect(\n",
    "        host=host,\n",
    "        port=port,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        database=database\n",
    ")\n",
    "    \n",
    "insert_query = \"\"\"\n",
    "        INSERT INTO {} ({})\n",
    "        VALUES ({})\n",
    "        \"\"\".format(table, ', '.join(insert_cols), ('%s, '*len(insert_cols))[:-2])\n",
    "\n",
    "update_query = \"\"\"\n",
    "        UPDATE {}\n",
    "        SET  statistical_num=%s, correct_time=%s\n",
    "        WHERE vin=%s and statistical_date=%s and content=%s and voiceName=%s and voiceType=%s and engineType=%s\n",
    "        \"\"\".format(table)\n",
    "\n",
    "with connection.cursor() as cursor:\n",
    "    # 插入操作\n",
    "    if insert_count == 0:\n",
    "        pass\n",
    "    elif insert_count == 1:\n",
    "        cursor.execute(insert_query, insert_data)\n",
    "    else:\n",
    "        cursor.executemany(insert_query, insert_data)\n",
    "\n",
    "    # 更新操作\n",
    "    if update_count == 0:\n",
    "        pass\n",
    "    elif update_count == 1:\n",
    "        cursor.execute(update_query, update_data)\n",
    "    else:\n",
    "        cursor.executemany(update_query, update_data)\n",
    "        \n",
    "# 提交事务\n",
    "connection.commit()\n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
