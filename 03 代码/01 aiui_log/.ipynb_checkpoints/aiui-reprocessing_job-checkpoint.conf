# 命名agent
a2.sources = r2
a2.sinks = k2
a2.channels = c2

# 配置source
a2.sources.r2.type = TAILDIR
# 监控检查点，用于保存检查到文件的哪个位置
a2.sources.r2.positionFile = position/aiui-reprocessing_taildir_position.json 
# 监控组，可以监控多个文件目录组/文件
a2.sources.r2.filegroups = f2
# 正则写包含log的文件
a2.sources.r2.filegroups.f2 = /home/hdfs/data/aiui/platform/log/aiui-reprocessing.*log.*

# 配置sink
a2.sinks.k2.type = hdfs
# 存放目录带时间信息
a2.sinks.k2.hdfs.path = hdfs://htwcluster/warehouse/ods/flume/aiui-reprocessing/%Y-%m-%d    
# 前缀
a2.sinks.k2.hdfs.filePrefix = aiui-reprocessing-
# 上传文件后缀
a2.sinks.k2.hdfs.fileSuffix = .log
# 是否按照实际滚动文件夹
# a2.sinks.k2.hdfs.round = true
# 多少时间单位创建一个新的文件夹
# a2.sinks.k2.hdfs.roundValue = 1
# a2.sinks.k2.hdfs.roundUnit = day
# 是否使用本地时间戳
a2.sinks.k2.hdfs.useLocalTimeStamp = true
# 积攒多少个Event才flush到HDFS一次
a2.sinks.k2.hdfs.batchSize = 10000
# 文件类型：SequenceFile, DataStream, CompressedStream
a2.sinks.k2.hdfs.fileType = DataStream
a2.sinks.k2.hdfs.writeFormat = Text
# 时间滚动：经过多久将临时文件滚动为目标文件，0为取消时间滚动
a2.sinks.k2.hdfs.rollInterval = 300
# 大小滚动：当临时文件达到该大小时，滚动成目标文件，单位：byte
a2.sinks.k2.hdfs.rollSize = 130000000
# 数量滚动：当Event数据达到该数量时，将临时文件滚动生成目标文件
a2.sinks.k2.hdfs.rollCount = 0

# chanel类型
a2.channels.c2.type = file
# 暂存最大事件数
# a2.channels.c2.capacity = 10000
# 一次出去的事件数
# a2.channels.c2.transactionCapacity =2000
# 进入/出去channel的事件的等待超时时间
# a2.channels.c2.keep-alive = 30
a2.channels.c2.checkpointDir = /tmp/flume/checkpoint/aiui-reprocessing
a2.channels.c2.dataDirs = /tmp/flume/data/aiui-reprocessing

# 用点线把这些任务连
a2.sources.r2.channels = c2
a2.sinks.k2.channel    = c2

